{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7657795,"sourceType":"datasetVersion","datasetId":4464890}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":null,"end_time":null,"environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-02-19T19:02:39.072135","version":"2.5.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Framework for predictions and portfolio forming","metadata":{"papermill":{"duration":0.015473,"end_time":"2024-02-19T19:02:42.059816","exception":false,"start_time":"2024-02-19T19:02:42.044343","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\npd.set_option('display.max_columns', None)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime as dt\nimport time\n\n# import the parquet library\nimport pyarrow.parquet as pq\n\n# import model libraries\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, accuracy_score","metadata":{"execution":{"iopub.execute_input":"2024-02-19T19:02:42.090780Z","iopub.status.busy":"2024-02-19T19:02:42.089986Z","iopub.status.idle":"2024-02-19T19:02:45.078375Z","shell.execute_reply":"2024-02-19T19:02:45.077234Z"},"papermill":{"duration":3.007452,"end_time":"2024-02-19T19:02:45.081642","exception":false,"start_time":"2024-02-19T19:02:42.074190","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load 'basemodel.parquet'\n#df = pd.read_parquet('basemodel.parquet')\ndf = pd.read_parquet('/kaggle/input/sign-prediction-datasets/basemodel.parquet')\nprediction_cols = []\ndf.head()","metadata":{"execution":{"iopub.execute_input":"2024-02-19T19:02:45.113341Z","iopub.status.busy":"2024-02-19T19:02:45.111797Z","iopub.status.idle":"2024-02-19T19:02:46.437578Z","shell.execute_reply":"2024-02-19T19:02:46.436448Z"},"papermill":{"duration":1.344359,"end_time":"2024-02-19T19:02:46.440777","exception":false,"start_time":"2024-02-19T19:02:45.096418","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# select the columns to be used for prediction\nX_col = ['bull_D', 'bear_D', 'bull_W', 'bear_W', 'bull_M', 'bear_M', 'LMKT', 'IVOL']","metadata":{"execution":{"iopub.execute_input":"2024-02-19T19:02:46.472934Z","iopub.status.busy":"2024-02-19T19:02:46.472495Z","iopub.status.idle":"2024-02-19T19:02:46.477289Z","shell.execute_reply":"2024-02-19T19:02:46.476200Z"},"papermill":{"duration":0.024105,"end_time":"2024-02-19T19:02:46.479436","exception":false,"start_time":"2024-02-19T19:02:46.455331","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert 'date' to datetime format (if not already done) and sort the DataFrame\ndf['date'] = pd.to_datetime(df['date'])\ndf.sort_values(by='date', inplace=True)\ndf.reset_index(drop=True, inplace=True)\n\n# Create a 'year' column based on the 'date' column\ndf['year'] = df['date'].dt.year","metadata":{"execution":{"iopub.execute_input":"2024-02-19T19:02:46.510215Z","iopub.status.busy":"2024-02-19T19:02:46.509444Z","iopub.status.idle":"2024-02-19T19:02:47.697159Z","shell.execute_reply":"2024-02-19T19:02:47.695755Z"},"papermill":{"duration":1.207061,"end_time":"2024-02-19T19:02:47.700981","exception":false,"start_time":"2024-02-19T19:02:46.493920","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Paper Replication - OLS and Logit, Expanding Window - No Hyperparameters\n- They start with out of sample forecasting in 1932\n- models will be named model_default","metadata":{"papermill":{"duration":0.016515,"end_time":"2024-02-19T19:02:47.740194","exception":false,"start_time":"2024-02-19T19:02:47.723679","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### Linear Regression (Pooled OLS)","metadata":{"papermill":{"duration":0.014778,"end_time":"2024-02-19T19:02:47.770000","exception":false,"start_time":"2024-02-19T19:02:47.755222","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# My Experiments","metadata":{"papermill":{"duration":0.020918,"end_time":"2024-02-19T19:04:20.843549","exception":false,"start_time":"2024-02-19T19:04:20.822631","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Machine Learning - Hyperparameter Tuning included in the process\n- models to be named 'model_clas/reg_exp/roll'","metadata":{"papermill":{"duration":0.021349,"end_time":"2024-02-19T19:04:20.886541","exception":false,"start_time":"2024-02-19T19:04:20.865192","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### Expanding Window Estimation","metadata":{"papermill":{"duration":0.021379,"end_time":"2024-02-19T19:04:20.929985","exception":false,"start_time":"2024-02-19T19:04:20.908606","status":"completed"},"tags":[]}},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.execute_input":"2024-02-19T19:44:46.063122Z","iopub.status.busy":"2024-02-19T19:44:46.062214Z"},"papermill":{"duration":null,"end_time":null,"exception":false,"start_time":"2024-02-19T19:44:46.011892","status":"running"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### First expanding, then rolling\nstart predicting for 1932, expand the window until you reach X years, then roll it","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"markdown","source":"##### 5 year window","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"# set the length of the rolling window\nrolling_window = 5 # years","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#################################\n# RANDOM FOREST REGRESSION\n#################################\nstart_time2 = time.time()  # Start timing\n\nmodel_name = 'RF_reg_roll5'  # Name for storing Random Forest regression predictions\n\n# Predefined set of values for hyperparameter tuning\nHP1 = [10, 50, 100]  # Possible values for n_estimators\nHP2 = [5, 10, 15]  # Possible values for max_depth\n\n# Update the column name for storing Random Forest regression predictions\ndf[model_name] = np.nan\n\n# Ensure the new column is in the prediction_cols list\nif model_name not in prediction_cols:\n    prediction_cols.append(model_name)\n\n# Define the start year for modeling based on having at least 7 years of data\nstart_modeling_year = df['year'].min() + 7\n\n\nfor year in range(start_modeling_year, df['year'].max() + 1):\n    start_time = time.time()  # Start timing\n    \n    # Determine the start year of the training window based on the current year\n    train_start_year = max(year - rolling_window, df['year'].min())  # Ensure it does not go below the earliest year\n    \n    # Select the training data based on the calculated start year\n    train_data = df[(df['year'] >= train_start_year) & (df['year'] < year)]\n    \n    # Split training data into actual training and tuning sets\n    # Use the last year of the training data for tuning\n    tuning_data = train_data[train_data['year'] == year - 1]\n    actual_train_data = train_data[train_data['year'] < year - 1]\n    \n    X_train = actual_train_data[X_col]\n    y_train = actual_train_data['y']\n    \n    X_tune = tuning_data[X_col]\n    y_tune = tuning_data['y']\n    \n    best_HP1 = None\n    best_HP2 = None\n    best_mse = float('inf')  # Initialize with infinity\n    \n    # Hyperparameter tuning\n    for hp1 in HP1:\n        for hp2 in HP2:\n            model = RandomForestRegressor(n_estimators=hp1, max_depth=hp2, random_state=42, n_jobs = -1)\n            model.fit(X_train, y_train)\n            predictions = model.predict(X_tune)  # Predict continuous values\n            mse = mean_squared_error(y_tune, predictions)  # Calculate MSE\n            \n            if mse < best_mse:  # Lower MSE is better\n                best_mse = mse\n                best_HP1 = hp1\n                best_HP2 = hp2\n    \n    # Retrain on the entire training window (excluding tuning year) with the best hyperparameters\n    model = RandomForestRegressor(n_estimators=best_HP1, max_depth=best_HP2, random_state=42, n_jobs = -1)\n    model.fit(X_train, y_train)\n    \n    # Predict for the next year\n    next_year_data = df[df['year'] == year]\n    X_next_year = next_year_data[X_col]\n    \n    if not X_next_year.empty:\n        next_year_predictions = model.predict(X_next_year)  # Predict continuous values\n        df.loc[df['year'] == year, model_name] = next_year_predictions\n    \n    end_time = time.time()  # End timing\n    iteration_time = end_time - start_time  # Calculate iteration time\n    \n    print(f\"Year {year} - Best n_estimators: {best_HP1}, Best max_depth: {best_HP2}, Best MSE: {round(best_mse,4)}, Time: {iteration_time:.2f} seconds\")\n\nend_time2 = time.time()\nprint(f\"Total time: {end_time2 - start_time2:.2f} seconds\")","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Forming Portfolios, Value-weighted portfolio returns","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"df.head()","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction_cols\n# prediction_cols = ['logit_default','OLS_default','logit_roll6','DT_reg_roll']","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"portfolio = df[['date', 'RET', 'ME', 'y'] + prediction_cols].copy()\nportfolio['date'] = pd.to_datetime(portfolio['date'])\n\n# drop rows with missing values\nportfolio.dropna(inplace=True)\n\nportfolio.head()","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"portfolio.tail()","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize an empty DataFrame to store value-weighted returns for each model\nvwreturns = pd.DataFrame(portfolio['date'].unique(), columns=['date'])  # Ensures all dates are included\n\nfor pred_col in prediction_cols:\n    # Calculate deciles for this prediction\n    decile_col = f'decile_{pred_col}'\n    portfolio[decile_col] = portfolio.groupby(['date'])[pred_col].transform(lambda x: pd.qcut(x, 10, labels=False, duplicates='drop'))\n    \n    # Determine position based on deciles\n    position_col = f'position_{pred_col}'\n    portfolio[position_col] = np.where(portfolio[decile_col] == 9, 1, np.where(portfolio[decile_col] == 0, -1, 0))\n    \n    # Calculate the value-weighted return for this prediction\n    vwret_col = f'vwreturn_{pred_col}'\n    vwreturns_temp = portfolio.groupby('date').apply(lambda x: np.sum(x['RET'] * x['ME'] * x[position_col]) / np.sum(x['ME'])).reset_index(name=vwret_col)\n    \n    # Merge the temporary value-weighted returns with the main vwreturns DataFrame\n    vwreturns = vwreturns.merge(vwreturns_temp, on='date', how='left')\n\n# Ensure the 'date' column is the first column and is sorted\nvwreturns = vwreturns.sort_values('date').reset_index(drop=True)\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vwreturns.head()","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Compare to market data","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"#market = pd.read_csv('FF3_clean.csv')\nmarket = pd.read_csv('/kaggle/input/sign-prediction-datasets/FF3_clean.csv')","metadata":{"execution":{"iopub.execute_input":"2024-02-19T19:01:33.669983Z","iopub.status.busy":"2024-02-19T19:01:33.669557Z","iopub.status.idle":"2024-02-19T19:01:33.689494Z","shell.execute_reply":"2024-02-19T19:01:33.688683Z","shell.execute_reply.started":"2024-02-19T19:01:33.669953Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"market.head()","metadata":{"execution":{"iopub.execute_input":"2024-02-19T19:01:37.527455Z","iopub.status.busy":"2024-02-19T19:01:37.527034Z","iopub.status.idle":"2024-02-19T19:01:37.540466Z","shell.execute_reply":"2024-02-19T19:01:37.539490Z","shell.execute_reply.started":"2024-02-19T19:01:37.527424Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a new 'Mkt' which is a sum of Mkt-RF and RF\nmarket['Mkt'] = market['Mkt-RF'] + market['RF']\n\n# divide all columns by 100 except 'date'\nmarket.iloc[:, 1:] = market.iloc[:, 1:] / 100\n\n#set the 'date' column to datetime format\nmarket['date'] = pd.to_datetime(market['date'])\n\n# merge the market data (only date and Mkt columns) with the vwreturns DataFrame\nvwreturns = vwreturns.merge(market[['date', 'Mkt']], on='date', how='left')\n\n# transform all columns (except 'date') to a log: log(x+1) and save the result as lvwreturns\nlvwreturns = vwreturns.copy()\nlvwreturns.iloc[:, 1:] = np.log(vwreturns.iloc[:, 1:] + 1)","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vwreturns.head()","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lvwreturns.head()","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lvwreturns.describe()","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## plot histograms of the value-weighted returns for each model and the market in lvwreturns\n#plt.figure(figsize=(12, round(len(prediction_cols)/2) * 5 ))\n#\n#for i, pred_col in enumerate(prediction_cols):\n#    plt.subplot(len(prediction_cols)/2 +1, 2, i+1)\n#    plt.hist(lvwreturns[f'vwreturn_{pred_col}'], bins=50, color='skyblue', edgecolor='black')\n#    plt.title(f'Value-Weighted Return - {pred_col}')\n#    plt.xlabel('Value-Weighted Return')\n#    plt.ylabel('Frequency')\n#    # calculate mean, skewness and kurtosis and add their values to the plot as a text, aligning to the top right corner\n#    mean = lvwreturns[f'vwreturn_{pred_col}'].mean()\n#    skewness = lvwreturns[f'vwreturn_{pred_col}'].skew()\n#    kurtosis = lvwreturns[f'vwreturn_{pred_col}'].kurtosis()\n#\n#    plt.text(0.95, 0.95, f'Mean: {mean:.4f}\\nSkewness: {skewness:.4f}\\nKurtosis: {kurtosis:.4f}', ha='right', va='top', transform=plt.gca().transAxes)\n#\n#\n#\n#plt.subplot(round(len(prediction_cols)/2) +1, 2, len(prediction_cols)+1)\n#plt.title('Value-Weighted Return - Market')\n#plt.xlabel('Value-Weighted Return')\n#plt.ylabel('Frequency')\n#plt.hist(lvwreturns['Mkt'], bins=50, color='skyblue', edgecolor='black')\n#mean = lvwreturns['Mkt'].mean()\n#skewness = lvwreturns['Mkt'].skew()\n#kurtosis = lvwreturns['Mkt'].kurtosis()\n#plt.text(0.95, 0.95, f'Mean: {mean:.4f}\\nSkewness: {skewness:.4f}\\nKurtosis: {kurtosis:.4f}', ha='right', va='top', transform=plt.gca().transAxes)\n#\n#plt.tight_layout()\n#plt.show()\n#","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## plot cumulative sums of the value-weighted log returns\n#plt.figure(figsize=(12, 6))\n#plt.plot(lvwreturns['date'], lvwreturns.iloc[:, 1:].cumsum())\n#plt.title('Cumulative Value-Weighted Log Returns')\n#plt.xlabel('Date')\n#plt.ylabel('Cumulative Value-Weighted Log Returns')\n#plt.legend(prediction_cols + ['Market'])\n#plt.show()\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save the lvwreturns and portfolio DataFrame to a parquet file into 'outputs' folder\n\n# for reproducibility and visualization purposes\nlvwreturns.to_parquet('base_lvwreturns_reg23.parquet')\nportfolio.to_parquet('base_portfolio_reg23.parquet')\n\n# save vwreturns DataFrame to a .dta file into 'outputs' folder\n#vwreturns.to_stata('outputs/vwreturns.dta') # for backtasting in R - we need normal returns, not log returns\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]}]}